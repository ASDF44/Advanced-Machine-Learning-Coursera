### Resources
* Explanation of Random Forest: https://www.datasciencecentral.com/profiles/blogs/random-forests-explained-intuitively
* Explanation/Demonstration of Gradient Boosting: http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html
* Example of kNN: https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/

### Basic ML algorithms:
* Linear Model
* Tree based
* kNN
* Neural Networks

### Hardware/Software Setup
* Preferable
    * 16+ GB RAM
    * 4+ Cores
* Good
    * 32+ GB RAM
    * 6+ Cores
* Cloud options
    * Amazon AWS
    * Microsoft Azure
    * Google Cloud
* Python Software stack
    * NumPy
    * Pandas
    * Scikit Learn
    * Matplotlib

### Overview of methods
* Scikit-Learn (or sklearn) library: https://scikit-learn.org/
* Overview of k-NN (sklearn's documentation): https://scikit-learn.org/stable/modules/neighbors.html
* Overview of Linear Models (sklearn's documentation): https://scikit-learn.org/stable/modules/linear_model.html
* Overview of Decision Trees (sklearn's documentation): https://scikit-learn.org/stable/modules/tree.html
* Overview of algorithms and parameters in H2O documentation: http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html

### Competitions' Platforms
- Kaggle
- DrivenData
- CrowdAnalityx
- CodaLab
- DataScienceChallenge.net
- Datascience.net
- Single-competition sites (like KDD, VizDoom)

### Machine Learning Libraries
* Scikit Learn
* Vowpal Wabbit - https://github.com/VowpalWabbit/vowpal_wabbit
* XGBoost - https://github.com/dmlc/xgboost
* LightGBM - https://github.com/Microsoft/LightGBM
* TensorFlow - https://www.tensorflow.org
* Keras - https://keras.io/
* MXNet - http://mxnet.incubator.apache.org/
* PyTorch - https://pytorch.org/
* Lasagne - https://lasagne.readthedocs.io/en/latest/